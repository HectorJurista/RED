import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from imblearn.over_sampling import SMOTE
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Cargar los datos
df = pd.read_csv('datos_red_neuronal.csv')  # Reemplaza con tu archivo

# 2. Exploración rápida
print(df.info())
print(df.describe())
print(df['clase'].value_counts())  # Suponiendo que la variable objetivo es 'clase'

# 3. Limpieza de datos
df = df.drop_duplicates()
df = df.dropna()  # O también puedes imputar: df.fillna(method='ffill', inplace=True)

# 4. Separar variables
X = df.drop('clase', axis=1)
y = df['clase']

# 5. Selección de características relevantes
selector = SelectKBest(score_func=f_classif, k=10)  # Selecciona las 10 mejores
X_selected = selector.fit_transform(X, y)
selected_columns = X.columns[selector.get_support()]
X = pd.DataFrame(X_selected, columns=selected_columns)

# 6. Balancear las clases (oversampling con SMOTE)
sm = SMOTE(random_state=42)
X_bal, y_bal = sm.fit_resample(X, y)
print("Distribución balanceada:", y_bal.value_counts())

# 7. Normalización
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_bal)

# 8. División de datos
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_bal, test_size=0.2, random_state=42)

# 9. Visualización opcional
sns.countplot(x=y_bal)
plt.title("Distribución balanceada de clases")
plt.show()

# Listo para usar en una red neuronal
print("Datos preprocesados para la red:")
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
